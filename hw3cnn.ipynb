{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "JIWQ_uMLqmQL"
   },
   "source": [
    "# Home 3: Build a CNN for image recognition.\n",
    "\n",
    "### Name: Colby Chaffin CWID: 10410591\n",
    "### \"I pledge my honor that I have abided by the Stevens Honor System\" -cchaffin\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "XdrEY2oyqmQM"
   },
   "source": [
    "## 0. You will do the following:\n",
    "\n",
    "1. Read, complete, and run my code.\n",
    "\n",
    "2. **Make substantial improvements** to maximize the accurcy.\n",
    "    \n",
    "3. Convert the .IPYNB file to .HTML file.\n",
    "\n",
    "    * The HTML file must contain the code and the output after execution.\n",
    "    \n",
    "4. Upload this .HTML file to your Github repo.\n",
    "\n",
    "4. Submit the link to this .HTML file to Canvas.\n",
    "\n",
    "    * Example: https://github.com/wangshusen/CS583A-2019Spring/blob/master/homework/HM3/cnn.html\n",
    "\n",
    "\n",
    "## Requirements:\n",
    "\n",
    "1. You can use whatever CNN architecture, including VGG, Inception, and ResNet. However, you must build the networks layer by layer. You must NOT import the archetectures from ```keras.applications```.\n",
    "\n",
    "2. Make sure ```BatchNormalization``` is between a ```Conv```/```Dense``` layer and an ```activation``` layer.\n",
    "\n",
    "3. If you want to regularize a ```Conv```/```Dense``` layer, you should place a ```Dropout``` layer **before** the ```Conv```/```Dense``` layer.\n",
    "\n",
    "4. An accuracy above 70% is considered reasonable. An accuracy above 80% is considered good. Without data augmentation, achieving 80% accuracy is difficult."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "gczI-pxDqmQM"
   },
   "source": [
    "## 1. Data preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "f0-l-nKYqmQN"
   },
   "source": [
    "### 1.1. Load data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 153
    },
    "colab_type": "code",
    "id": "B_yl4RahqmQO",
    "outputId": "8309bd20-548e-48fa-dc02-b0e1ff18ff4b"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz\n",
      "170500096/170498071 [==============================] - 3s 0us/step\n",
      "shape of x_train: (50000, 32, 32, 3)\n",
      "shape of y_train: (50000, 1)\n",
      "shape of x_test: (10000, 32, 32, 3)\n",
      "shape of y_test: (10000, 1)\n",
      "number of classes: 10\n"
     ]
    }
   ],
   "source": [
    "from keras.datasets import cifar10\n",
    "from keras.utils import to_categorical\n",
    "from keras.wrappers.scikit_learn import KerasClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from keras import optimizers\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "(x_train, y_train), (x_test, y_test) = cifar10.load_data()\n",
    "\n",
    "print('shape of x_train: ' + str(x_train.shape))\n",
    "print('shape of y_train: ' + str(y_train.shape))\n",
    "print('shape of x_test: ' + str(x_test.shape))\n",
    "print('shape of y_test: ' + str(y_test.shape))\n",
    "print('number of classes: ' + str(np.max(y_train) - np.min(y_train) + 1))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ZaRardsNqmQT"
   },
   "source": [
    "### 1.2. One-hot encode the labels\n",
    "\n",
    "In the input, a label is a scalar in $\\{0, 1, \\cdots , 9\\}$. One-hot encode transform such a scalar to a $10$-dim vector. E.g., a scalar ```y_train[j]=3``` is transformed to the vector ```y_train_vec[j]=[0, 0, 0, 1, 0, 0, 0, 0, 0, 0]```.\n",
    "\n",
    "1. Define a function ```to_one_hot``` that transforms an $n\\times 1$ array to a $n\\times 10$ matrix.\n",
    "\n",
    "2. Apply the function to ```y_train``` and ```y_test```."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 85
    },
    "colab_type": "code",
    "id": "9s2Ar_rEqmQU",
    "outputId": "a8de1429-11ec-4851-8372-fc474feeb082"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of y_train_vec: (50000, 10)\n",
      "Shape of y_test_vec: (10000, 10)\n",
      "[6]\n",
      "[0. 0. 0. 0. 0. 0. 1. 0. 0. 0.]\n"
     ]
    }
   ],
   "source": [
    "def to_one_hot(y, num_class=10):\n",
    "    return to_categorical(y, num_classes=num_class)\n",
    "\n",
    "y_train_vec = to_one_hot(y_train)\n",
    "y_test_vec = to_one_hot(y_test)\n",
    "\n",
    "print('Shape of y_train_vec: ' + str(y_train_vec.shape))\n",
    "print('Shape of y_test_vec: ' + str(y_test_vec.shape))\n",
    "\n",
    "print(y_train[0])\n",
    "print(y_train_vec[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "9kiJybqEqmQY"
   },
   "source": [
    "#### Remark: the outputs should be\n",
    "* Shape of y_train_vec: (50000, 10)\n",
    "* Shape of y_test_vec: (10000, 10)\n",
    "* [6]\n",
    "* [0. 0. 0. 0. 0. 0. 1. 0. 0. 0.]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "LkG2tmokqmQY"
   },
   "source": [
    "### 1.3. Randomly partition the training set to training and validation sets\n",
    "\n",
    "Randomly partition the 50K training samples to 2 sets:\n",
    "* a training set containing 40K samples\n",
    "* a validation set containing 10K samples\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 85
    },
    "colab_type": "code",
    "id": "3Gm8kwJAqmQZ",
    "outputId": "1ae4c00c-73db-4ec3-ae73-a6efaccebfa4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of x_tr: (40000, 32, 32, 3)\n",
      "Shape of y_tr: (40000, 10)\n",
      "Shape of x_val: (10000, 32, 32, 3)\n",
      "Shape of y_val: (10000, 10)\n"
     ]
    }
   ],
   "source": [
    "rand_indices = np.random.permutation(50000)\n",
    "train_indices = rand_indices[0:40000]\n",
    "valid_indices = rand_indices[40000:50000]\n",
    "\n",
    "x_val = x_train[valid_indices, :]\n",
    "y_val = y_train_vec[valid_indices, :]\n",
    "\n",
    "x_tr = x_train[train_indices, :]\n",
    "y_tr = y_train_vec[train_indices, :]\n",
    "\n",
    "print('Shape of x_tr: ' + str(x_tr.shape))\n",
    "print('Shape of y_tr: ' + str(y_tr.shape))\n",
    "print('Shape of x_val: ' + str(x_val.shape))\n",
    "print('Shape of y_val: ' + str(y_val.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "De-L87EvqmQd"
   },
   "source": [
    "## 2. Build a CNN and tune its hyper-parameters\n",
    "\n",
    "1. Build a convolutional neural network model\n",
    "2. Use the validation data to tune the hyper-parameters (e.g., network structure, and optimization algorithm)\n",
    "    * Do NOT use test data for hyper-parameter tuning!!!\n",
    "3. Try to achieve a validation accuracy as high as possible."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "68_m5QcJqmQd"
   },
   "source": [
    "### Remark: \n",
    "\n",
    "The following CNN is just an example. You are supposed to make **substantial improvements** such as:\n",
    "* Add more layers.\n",
    "* Use regularizations, e.g., dropout.\n",
    "* Use batch normalization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "lwOrRGHLqmQe"
   },
   "outputs": [],
   "source": [
    "from keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, BatchNormalization, Activation, Dropout\n",
    "from keras.models import Sequential\n",
    "from keras import regularizers\n",
    "\n",
    "\n",
    "from keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Activation, BatchNormalization, Dropout\n",
    "from keras.models import Sequential\n",
    "\n",
    "def build_model():\n",
    "    model = Sequential()\n",
    "    model.add(Conv2D(32, (3, 3), padding='same', input_shape=(32, 32, 3)))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Activation(\"relu\"))\n",
    "    model.add(MaxPooling2D((2, 2)))\n",
    "    \n",
    "    model.add(Conv2D(64, (3, 3), padding='same'))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Activation(\"relu\"))\n",
    "    model.add(MaxPooling2D((2, 2)))\n",
    "    \n",
    "    model.add(Conv2D(128, (3, 3), padding='same'))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Activation(\"relu\"))\n",
    "    model.add(MaxPooling2D((2, 2)))\n",
    "    \n",
    "    model.add(Dropout(0.5))\n",
    "    \n",
    "    model.add(Conv2D(256, (3, 3), padding='same'))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Activation(\"relu\"))\n",
    "    model.add(MaxPooling2D((2, 2)))\n",
    "    \n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(128, activation='relu'))\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(Dense(128, activation='relu'))\n",
    "    model.add(Dense(10, activation='softmax'))\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Z1BPzMCfXGSm"
   },
   "source": [
    "**Data Augmentation** - Using data augmentation to increase the number of training samples we are able to use for training the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "MOwmno_qXeQJ"
   },
   "outputs": [],
   "source": [
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "# data augmentation\n",
    "datagen = ImageDataGenerator(\n",
    "    rotation_range=20,\n",
    "    width_shift_range=0.2,\n",
    "    height_shift_range=0.2,\n",
    "    horizontal_flip=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "lF3ViLfzXjDa"
   },
   "source": [
    "**Hyperparams**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "VoqQwnqEqmQk"
   },
   "outputs": [],
   "source": [
    "learning_rate = 1E-4 # to be tuned!\n",
    "batch_size = 64\n",
    "epochs = 50\n",
    "steps_per_epoch = len(x_tr) // batch_size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "9MwBD-16Xn4P"
   },
   "source": [
    "**Tuning Hyperparameters for more optimal performance**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 2757
    },
    "colab_type": "code",
    "id": "zujpqGKdqmQm",
    "outputId": "43cbaf36-c6ac-4053-9cf3-5154d18bfeb1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3445: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_1 (Conv2D)            (None, 32, 32, 32)        896       \n",
      "_________________________________________________________________\n",
      "batch_normalization_1 (Batch (None, 32, 32, 32)        128       \n",
      "_________________________________________________________________\n",
      "activation_1 (Activation)    (None, 32, 32, 32)        0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2 (None, 16, 16, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_2 (Conv2D)            (None, 16, 16, 64)        18496     \n",
      "_________________________________________________________________\n",
      "batch_normalization_2 (Batch (None, 16, 16, 64)        256       \n",
      "_________________________________________________________________\n",
      "activation_2 (Activation)    (None, 16, 16, 64)        0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_2 (MaxPooling2 (None, 8, 8, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv2d_3 (Conv2D)            (None, 8, 8, 128)         73856     \n",
      "_________________________________________________________________\n",
      "batch_normalization_3 (Batch (None, 8, 8, 128)         512       \n",
      "_________________________________________________________________\n",
      "activation_3 (Activation)    (None, 8, 8, 128)         0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_3 (MaxPooling2 (None, 4, 4, 128)         0         \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 4, 4, 128)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_4 (Conv2D)            (None, 4, 4, 256)         295168    \n",
      "_________________________________________________________________\n",
      "batch_normalization_4 (Batch (None, 4, 4, 256)         1024      \n",
      "_________________________________________________________________\n",
      "activation_4 (Activation)    (None, 4, 4, 256)         0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_4 (MaxPooling2 (None, 2, 2, 256)         0         \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 1024)              0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 128)               131200    \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 128)               16512     \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 10)                1290      \n",
      "=================================================================\n",
      "Total params: 539,338\n",
      "Trainable params: 538,378\n",
      "Non-trainable params: 960\n",
      "_________________________________________________________________\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "Epoch 1/50\n",
      "625/625 [==============================] - 22s 36ms/step - loss: 2.0430 - acc: 0.2390 - val_loss: 1.7645 - val_acc: 0.3628\n",
      "Epoch 2/50\n",
      "625/625 [==============================] - 18s 29ms/step - loss: 1.7965 - acc: 0.3246 - val_loss: 1.5789 - val_acc: 0.4235\n",
      "Epoch 3/50\n",
      "625/625 [==============================] - 20s 32ms/step - loss: 1.7028 - acc: 0.3688 - val_loss: 1.6509 - val_acc: 0.3965\n",
      "Epoch 4/50\n",
      "625/625 [==============================] - 19s 31ms/step - loss: 1.6382 - acc: 0.3974 - val_loss: 1.5220 - val_acc: 0.4587\n",
      "Epoch 5/50\n",
      "625/625 [==============================] - 18s 29ms/step - loss: 1.5817 - acc: 0.4184 - val_loss: 1.4853 - val_acc: 0.4720\n",
      "Epoch 6/50\n",
      "625/625 [==============================] - 18s 29ms/step - loss: 1.5392 - acc: 0.4359 - val_loss: 1.3837 - val_acc: 0.4978\n",
      "Epoch 7/50\n",
      "625/625 [==============================] - 20s 32ms/step - loss: 1.5008 - acc: 0.4536 - val_loss: 1.4610 - val_acc: 0.4835\n",
      "Epoch 8/50\n",
      "625/625 [==============================] - 18s 29ms/step - loss: 1.4622 - acc: 0.4672 - val_loss: 1.4497 - val_acc: 0.4960\n",
      "Epoch 9/50\n",
      "625/625 [==============================] - 18s 30ms/step - loss: 1.4345 - acc: 0.4780 - val_loss: 1.2288 - val_acc: 0.5475\n",
      "Epoch 10/50\n",
      "625/625 [==============================] - 18s 30ms/step - loss: 1.4102 - acc: 0.4907 - val_loss: 1.2681 - val_acc: 0.5431\n",
      "Epoch 11/50\n",
      "625/625 [==============================] - 20s 32ms/step - loss: 1.3829 - acc: 0.4983 - val_loss: 1.3037 - val_acc: 0.5330\n",
      "Epoch 12/50\n",
      "625/625 [==============================] - 18s 29ms/step - loss: 1.3622 - acc: 0.5088 - val_loss: 1.1679 - val_acc: 0.5797\n",
      "Epoch 13/50\n",
      "625/625 [==============================] - 18s 29ms/step - loss: 1.3439 - acc: 0.5167 - val_loss: 1.2592 - val_acc: 0.5575\n",
      "Epoch 14/50\n",
      "625/625 [==============================] - 18s 29ms/step - loss: 1.3206 - acc: 0.5260 - val_loss: 1.3531 - val_acc: 0.5476\n",
      "Epoch 15/50\n",
      "625/625 [==============================] - 21s 33ms/step - loss: 1.3054 - acc: 0.5326 - val_loss: 1.2292 - val_acc: 0.5771\n",
      "Epoch 16/50\n",
      "625/625 [==============================] - 19s 31ms/step - loss: 1.2939 - acc: 0.5373 - val_loss: 1.3685 - val_acc: 0.5389\n",
      "Epoch 17/50\n",
      "625/625 [==============================] - 18s 30ms/step - loss: 1.2728 - acc: 0.5417 - val_loss: 1.2092 - val_acc: 0.5782\n",
      "Epoch 18/50\n",
      "625/625 [==============================] - 18s 30ms/step - loss: 1.2586 - acc: 0.5514 - val_loss: 1.0942 - val_acc: 0.6142\n",
      "Epoch 19/50\n",
      "625/625 [==============================] - 18s 30ms/step - loss: 1.2454 - acc: 0.5542 - val_loss: 1.2170 - val_acc: 0.5810\n",
      "Epoch 20/50\n",
      "625/625 [==============================] - 20s 32ms/step - loss: 1.2304 - acc: 0.5598 - val_loss: 1.1103 - val_acc: 0.6072\n",
      "Epoch 21/50\n",
      "625/625 [==============================] - 18s 29ms/step - loss: 1.2221 - acc: 0.5650 - val_loss: 1.0548 - val_acc: 0.6297\n",
      "Epoch 22/50\n",
      "625/625 [==============================] - 19s 30ms/step - loss: 1.2074 - acc: 0.5686 - val_loss: 1.1308 - val_acc: 0.5880\n",
      "Epoch 23/50\n",
      "625/625 [==============================] - 18s 29ms/step - loss: 1.1953 - acc: 0.5730 - val_loss: 1.0134 - val_acc: 0.6466\n",
      "Epoch 24/50\n",
      "625/625 [==============================] - 20s 32ms/step - loss: 1.1785 - acc: 0.5788 - val_loss: 1.0757 - val_acc: 0.6256\n",
      "Epoch 25/50\n",
      "625/625 [==============================] - 18s 30ms/step - loss: 1.1753 - acc: 0.5842 - val_loss: 1.0851 - val_acc: 0.6186\n",
      "Epoch 26/50\n",
      "625/625 [==============================] - 19s 30ms/step - loss: 1.1654 - acc: 0.5868 - val_loss: 1.0649 - val_acc: 0.6219\n",
      "Epoch 27/50\n",
      "625/625 [==============================] - 18s 29ms/step - loss: 1.1500 - acc: 0.5912 - val_loss: 1.0265 - val_acc: 0.6410\n",
      "Epoch 28/50\n",
      "625/625 [==============================] - 20s 31ms/step - loss: 1.1421 - acc: 0.5968 - val_loss: 0.9803 - val_acc: 0.6547\n",
      "Epoch 29/50\n",
      "625/625 [==============================] - 19s 30ms/step - loss: 1.1334 - acc: 0.5978 - val_loss: 1.0610 - val_acc: 0.6309\n",
      "Epoch 30/50\n",
      "625/625 [==============================] - 18s 29ms/step - loss: 1.1236 - acc: 0.6032 - val_loss: 1.1007 - val_acc: 0.6329\n",
      "Epoch 31/50\n",
      "625/625 [==============================] - 20s 31ms/step - loss: 1.1229 - acc: 0.6041 - val_loss: 1.0531 - val_acc: 0.6329\n",
      "Epoch 32/50\n",
      "625/625 [==============================] - 20s 31ms/step - loss: 1.1076 - acc: 0.6111 - val_loss: 1.0354 - val_acc: 0.6366\n",
      "Epoch 33/50\n",
      "625/625 [==============================] - 19s 31ms/step - loss: 1.1042 - acc: 0.6101 - val_loss: 0.9915 - val_acc: 0.6575\n",
      "Epoch 34/50\n",
      "625/625 [==============================] - 18s 30ms/step - loss: 1.0928 - acc: 0.6155 - val_loss: 0.9604 - val_acc: 0.6642\n",
      "Epoch 35/50\n",
      "625/625 [==============================] - 18s 30ms/step - loss: 1.0894 - acc: 0.6170 - val_loss: 1.0381 - val_acc: 0.6463\n",
      "Epoch 36/50\n",
      "625/625 [==============================] - 19s 31ms/step - loss: 1.0757 - acc: 0.6198 - val_loss: 0.9298 - val_acc: 0.6832\n",
      "Epoch 37/50\n",
      "625/625 [==============================] - 20s 32ms/step - loss: 1.0704 - acc: 0.6257 - val_loss: 1.0080 - val_acc: 0.6598\n",
      "Epoch 38/50\n",
      "625/625 [==============================] - 19s 30ms/step - loss: 1.0549 - acc: 0.6306 - val_loss: 0.8907 - val_acc: 0.6927\n",
      "Epoch 39/50\n",
      "625/625 [==============================] - 19s 30ms/step - loss: 1.0561 - acc: 0.6302 - val_loss: 0.9955 - val_acc: 0.6531\n",
      "Epoch 40/50\n",
      "625/625 [==============================] - 19s 30ms/step - loss: 1.0556 - acc: 0.6276 - val_loss: 1.0182 - val_acc: 0.6442\n",
      "Epoch 41/50\n",
      "625/625 [==============================] - 20s 32ms/step - loss: 1.0495 - acc: 0.6307 - val_loss: 1.0414 - val_acc: 0.6413\n",
      "Epoch 42/50\n",
      "625/625 [==============================] - 19s 30ms/step - loss: 1.0451 - acc: 0.6342 - val_loss: 1.0291 - val_acc: 0.6446\n",
      "Epoch 43/50\n",
      "625/625 [==============================] - 18s 29ms/step - loss: 1.0326 - acc: 0.6386 - val_loss: 0.9792 - val_acc: 0.6552\n",
      "Epoch 44/50\n",
      "625/625 [==============================] - 18s 29ms/step - loss: 1.0279 - acc: 0.6384 - val_loss: 0.9738 - val_acc: 0.6521\n",
      "Epoch 45/50\n",
      "625/625 [==============================] - 20s 32ms/step - loss: 1.0135 - acc: 0.6431 - val_loss: 0.9401 - val_acc: 0.6795\n",
      "Epoch 46/50\n",
      "625/625 [==============================] - 18s 29ms/step - loss: 1.0134 - acc: 0.6466 - val_loss: 0.9338 - val_acc: 0.6728\n",
      "Epoch 47/50\n",
      "625/625 [==============================] - 18s 29ms/step - loss: 1.0142 - acc: 0.6443 - val_loss: 0.8410 - val_acc: 0.7059\n",
      "Epoch 48/50\n",
      "625/625 [==============================] - 20s 32ms/step - loss: 1.0054 - acc: 0.6473 - val_loss: 0.8983 - val_acc: 0.6935\n",
      "Epoch 49/50\n",
      "625/625 [==============================] - 19s 31ms/step - loss: 0.9997 - acc: 0.6524 - val_loss: 0.9393 - val_acc: 0.6704\n",
      "Epoch 50/50\n",
      "625/625 [==============================] - 19s 30ms/step - loss: 0.9879 - acc: 0.6545 - val_loss: 0.8377 - val_acc: 0.7066\n"
     ]
    }
   ],
   "source": [
    "model = build_model()\n",
    "model.summary()\n",
    "\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer=optimizers.RMSprop(lr=learning_rate),\n",
    "              metrics=['acc'])\n",
    "\n",
    "history = model.fit_generator(datagen.flow(x_tr, y_tr, batch_size=batch_size),\n",
    "                              steps_per_epoch=steps_per_epoch,\n",
    "                              epochs=epochs,\n",
    "                              validation_data=(x_val, y_val))\n",
    "model.save('cifar10.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "5IDjWKpcqmQp"
   },
   "outputs": [],
   "source": [
    "##history = model.fit(x_tr, y_tr, batch_size=32, epochs=10, validation_data=(x_val, y_val))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "HuoDX3aiXy2Q"
   },
   "source": [
    "**Plot results for Hyperparams**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 283
    },
    "colab_type": "code",
    "id": "4_Ap1HLUqmQs",
    "outputId": "b70c9963-bd4c-4940-dcad-d4e56201e8c4"
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYUAAAEKCAYAAAD9xUlFAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nKBhjjPH6f5HpnA3XmG3rAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "acc = history.history['acc']\n",
    "val_acc = history.history['val_acc']\n",
    "\n",
    "epochs = range(len(acc))\n",
    "\n",
    "plt.plot(epochs, acc, 'bo', label='Training acc')\n",
    "plt.plot(epochs, val_acc, 'r', label='Validation acc')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "sR-_1N5VqmQx"
   },
   "source": [
    "## 3. Train (again) and evaluate the model\n",
    "\n",
    "- To this end, you have found the \"best\" hyper-parameters. \n",
    "- Now, fix the hyper-parameters and train the network on the entire training set (all the 50K training samples)\n",
    "- Evaluate your model on the test set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "EZWwN4liqmQy"
   },
   "source": [
    "### 3.1. Train the model on the entire training set\n",
    "\n",
    "Why? Previously, you used 40K samples for training; you wasted 10K samples for the sake of hyper-parameter tuning. Now you already know the hyper-parameters, so why not using all the 50K samples for training?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 884
    },
    "colab_type": "code",
    "id": "ogzufUzOqmQz",
    "outputId": "d7d1e95b-8412-4bc6-d6e8-999add4d6dac"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_1 (Conv2D)            (None, 32, 32, 32)        896       \n",
      "_________________________________________________________________\n",
      "batch_normalization_1 (Batch (None, 32, 32, 32)        128       \n",
      "_________________________________________________________________\n",
      "activation_1 (Activation)    (None, 32, 32, 32)        0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2 (None, 16, 16, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_2 (Conv2D)            (None, 16, 16, 64)        18496     \n",
      "_________________________________________________________________\n",
      "batch_normalization_2 (Batch (None, 16, 16, 64)        256       \n",
      "_________________________________________________________________\n",
      "activation_2 (Activation)    (None, 16, 16, 64)        0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_2 (MaxPooling2 (None, 8, 8, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv2d_3 (Conv2D)            (None, 8, 8, 128)         73856     \n",
      "_________________________________________________________________\n",
      "batch_normalization_3 (Batch (None, 8, 8, 128)         512       \n",
      "_________________________________________________________________\n",
      "activation_3 (Activation)    (None, 8, 8, 128)         0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_3 (MaxPooling2 (None, 4, 4, 128)         0         \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 4, 4, 128)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_4 (Conv2D)            (None, 4, 4, 256)         295168    \n",
      "_________________________________________________________________\n",
      "batch_normalization_4 (Batch (None, 4, 4, 256)         1024      \n",
      "_________________________________________________________________\n",
      "activation_4 (Activation)    (None, 4, 4, 256)         0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_4 (MaxPooling2 (None, 2, 2, 256)         0         \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 1024)              0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 128)               131200    \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 128)               16512     \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 10)                1290      \n",
      "=================================================================\n",
      "Total params: 539,338\n",
      "Trainable params: 538,378\n",
      "Non-trainable params: 960\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "fin_model = build_model()\n",
    "\n",
    "fin_model.compile(loss='categorical_crossentropy',\n",
    "              optimizer=optimizers.RMSprop(lr=learning_rate),\n",
    "              metrics=['acc'])\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 5117
    },
    "colab_type": "code",
    "id": "y8LxJvuSqmQ2",
    "outputId": "571d63d5-fc91-4f24-f87d-6029765a39a4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/150\n",
      "625/625 [==============================] - 21s 33ms/step - loss: 2.0437 - acc: 0.2373\n",
      "Epoch 2/150\n",
      "625/625 [==============================] - 19s 30ms/step - loss: 1.8024 - acc: 0.3252\n",
      "Epoch 3/150\n",
      "625/625 [==============================] - 18s 29ms/step - loss: 1.7120 - acc: 0.3620\n",
      "Epoch 4/150\n",
      "625/625 [==============================] - 18s 29ms/step - loss: 1.6343 - acc: 0.3962\n",
      "Epoch 5/150\n",
      "625/625 [==============================] - 20s 31ms/step - loss: 1.5844 - acc: 0.4124\n",
      "Epoch 6/150\n",
      "625/625 [==============================] - 18s 29ms/step - loss: 1.5392 - acc: 0.4366\n",
      "Epoch 7/150\n",
      "625/625 [==============================] - 18s 29ms/step - loss: 1.5079 - acc: 0.4485\n",
      "Epoch 8/150\n",
      "625/625 [==============================] - 18s 29ms/step - loss: 1.4676 - acc: 0.4678\n",
      "Epoch 9/150\n",
      "625/625 [==============================] - 19s 30ms/step - loss: 1.4442 - acc: 0.4771\n",
      "Epoch 10/150\n",
      "625/625 [==============================] - 19s 30ms/step - loss: 1.4191 - acc: 0.4852\n",
      "Epoch 11/150\n",
      "625/625 [==============================] - 18s 29ms/step - loss: 1.3867 - acc: 0.4997\n",
      "Epoch 12/150\n",
      "625/625 [==============================] - 18s 29ms/step - loss: 1.3778 - acc: 0.5029\n",
      "Epoch 13/150\n",
      "625/625 [==============================] - 18s 29ms/step - loss: 1.3474 - acc: 0.5144\n",
      "Epoch 14/150\n",
      "625/625 [==============================] - 20s 31ms/step - loss: 1.3423 - acc: 0.5183\n",
      "Epoch 15/150\n",
      "625/625 [==============================] - 20s 32ms/step - loss: 1.3172 - acc: 0.5301\n",
      "Epoch 16/150\n",
      "625/625 [==============================] - 18s 29ms/step - loss: 1.3075 - acc: 0.5357\n",
      "Epoch 17/150\n",
      "625/625 [==============================] - 19s 31ms/step - loss: 1.2876 - acc: 0.5411\n",
      "Epoch 18/150\n",
      "625/625 [==============================] - 20s 32ms/step - loss: 1.2727 - acc: 0.5444\n",
      "Epoch 19/150\n",
      "625/625 [==============================] - 18s 29ms/step - loss: 1.2524 - acc: 0.5540\n",
      "Epoch 20/150\n",
      "625/625 [==============================] - 18s 29ms/step - loss: 1.2435 - acc: 0.5599\n",
      "Epoch 21/150\n",
      "625/625 [==============================] - 18s 29ms/step - loss: 1.2329 - acc: 0.5610\n",
      "Epoch 22/150\n",
      "625/625 [==============================] - 20s 32ms/step - loss: 1.2182 - acc: 0.5672\n",
      "Epoch 23/150\n",
      "625/625 [==============================] - 18s 29ms/step - loss: 1.2081 - acc: 0.5734\n",
      "Epoch 24/150\n",
      "625/625 [==============================] - 18s 29ms/step - loss: 1.2009 - acc: 0.5758\n",
      "Epoch 25/150\n",
      "625/625 [==============================] - 18s 29ms/step - loss: 1.1941 - acc: 0.5774\n",
      "Epoch 26/150\n",
      "625/625 [==============================] - 19s 30ms/step - loss: 1.1728 - acc: 0.5849\n",
      "Epoch 27/150\n",
      "625/625 [==============================] - 19s 31ms/step - loss: 1.1662 - acc: 0.5884\n",
      "Epoch 28/150\n",
      "625/625 [==============================] - 18s 29ms/step - loss: 1.1575 - acc: 0.5923\n",
      "Epoch 29/150\n",
      "625/625 [==============================] - 18s 29ms/step - loss: 1.1401 - acc: 0.5983\n",
      "Epoch 30/150\n",
      "625/625 [==============================] - 18s 29ms/step - loss: 1.1377 - acc: 0.6003\n",
      "Epoch 31/150\n",
      "625/625 [==============================] - 20s 32ms/step - loss: 1.1292 - acc: 0.6035\n",
      "Epoch 32/150\n",
      "625/625 [==============================] - 20s 32ms/step - loss: 1.1189 - acc: 0.6044\n",
      "Epoch 33/150\n",
      "625/625 [==============================] - 19s 31ms/step - loss: 1.1143 - acc: 0.6097\n",
      "Epoch 34/150\n",
      "625/625 [==============================] - 18s 29ms/step - loss: 1.1062 - acc: 0.6117\n",
      "Epoch 35/150\n",
      "625/625 [==============================] - 20s 32ms/step - loss: 1.0892 - acc: 0.6165\n",
      "Epoch 36/150\n",
      "625/625 [==============================] - 18s 29ms/step - loss: 1.0885 - acc: 0.6185\n",
      "Epoch 37/150\n",
      "625/625 [==============================] - 18s 29ms/step - loss: 1.0782 - acc: 0.6204\n",
      "Epoch 38/150\n",
      "625/625 [==============================] - 18s 29ms/step - loss: 1.0785 - acc: 0.6244\n",
      "Epoch 39/150\n",
      "625/625 [==============================] - 20s 31ms/step - loss: 1.0723 - acc: 0.6248\n",
      "Epoch 40/150\n",
      "625/625 [==============================] - 19s 30ms/step - loss: 1.0596 - acc: 0.6287\n",
      "Epoch 41/150\n",
      "625/625 [==============================] - 18s 29ms/step - loss: 1.0560 - acc: 0.6304\n",
      "Epoch 42/150\n",
      "625/625 [==============================] - 18s 29ms/step - loss: 1.0560 - acc: 0.6307\n",
      "Epoch 43/150\n",
      "625/625 [==============================] - 18s 29ms/step - loss: 1.0312 - acc: 0.6390\n",
      "Epoch 44/150\n",
      "625/625 [==============================] - 20s 32ms/step - loss: 1.0314 - acc: 0.6394\n",
      "Epoch 45/150\n",
      "625/625 [==============================] - 18s 29ms/step - loss: 1.0334 - acc: 0.6372\n",
      "Epoch 46/150\n",
      "625/625 [==============================] - 18s 29ms/step - loss: 1.0187 - acc: 0.6448\n",
      "Epoch 47/150\n",
      "625/625 [==============================] - 18s 29ms/step - loss: 1.0150 - acc: 0.6469\n",
      "Epoch 48/150\n",
      "625/625 [==============================] - 21s 33ms/step - loss: 1.0159 - acc: 0.6452\n",
      "Epoch 49/150\n",
      "625/625 [==============================] - 19s 30ms/step - loss: 1.0120 - acc: 0.6487\n",
      "Epoch 50/150\n",
      "625/625 [==============================] - 19s 30ms/step - loss: 1.0030 - acc: 0.6510\n",
      "Epoch 51/150\n",
      "625/625 [==============================] - 18s 29ms/step - loss: 0.9917 - acc: 0.6526\n",
      "Epoch 52/150\n",
      "625/625 [==============================] - 20s 32ms/step - loss: 0.9956 - acc: 0.6571\n",
      "Epoch 53/150\n",
      "625/625 [==============================] - 18s 29ms/step - loss: 0.9776 - acc: 0.6627\n",
      "Epoch 54/150\n",
      "625/625 [==============================] - 18s 29ms/step - loss: 0.9905 - acc: 0.6538\n",
      "Epoch 55/150\n",
      "625/625 [==============================] - 18s 29ms/step - loss: 0.9840 - acc: 0.6589\n",
      "Epoch 56/150\n",
      "625/625 [==============================] - 19s 30ms/step - loss: 0.9760 - acc: 0.6614\n",
      "Epoch 57/150\n",
      "625/625 [==============================] - 19s 30ms/step - loss: 0.9768 - acc: 0.6616\n",
      "Epoch 58/150\n",
      "625/625 [==============================] - 18s 29ms/step - loss: 0.9645 - acc: 0.6663\n",
      "Epoch 59/150\n",
      "625/625 [==============================] - 18s 29ms/step - loss: 0.9661 - acc: 0.6627\n",
      "Epoch 60/150\n",
      "625/625 [==============================] - 18s 29ms/step - loss: 0.9510 - acc: 0.6706\n",
      "Epoch 61/150\n",
      "625/625 [==============================] - 19s 31ms/step - loss: 0.9545 - acc: 0.6695\n",
      "Epoch 62/150\n",
      "625/625 [==============================] - 18s 29ms/step - loss: 0.9455 - acc: 0.6720\n",
      "Epoch 63/150\n",
      "625/625 [==============================] - 18s 29ms/step - loss: 0.9462 - acc: 0.6730\n",
      "Epoch 64/150\n",
      "625/625 [==============================] - 18s 29ms/step - loss: 0.9461 - acc: 0.6747\n",
      "Epoch 65/150\n",
      "625/625 [==============================] - 21s 33ms/step - loss: 0.9424 - acc: 0.6748\n",
      "Epoch 66/150\n",
      "625/625 [==============================] - 19s 30ms/step - loss: 0.9302 - acc: 0.6799\n",
      "Epoch 67/150\n",
      "625/625 [==============================] - 18s 29ms/step - loss: 0.9273 - acc: 0.6781\n",
      "Epoch 68/150\n",
      "625/625 [==============================] - 18s 29ms/step - loss: 0.9278 - acc: 0.6773\n",
      "Epoch 69/150\n",
      "625/625 [==============================] - 19s 30ms/step - loss: 0.9269 - acc: 0.6796\n",
      "Epoch 70/150\n",
      "625/625 [==============================] - 19s 31ms/step - loss: 0.9191 - acc: 0.6819\n",
      "Epoch 71/150\n",
      "625/625 [==============================] - 18s 29ms/step - loss: 0.9093 - acc: 0.6820\n",
      "Epoch 72/150\n",
      "625/625 [==============================] - 18s 29ms/step - loss: 0.9185 - acc: 0.6824\n",
      "Epoch 73/150\n",
      "625/625 [==============================] - 18s 29ms/step - loss: 0.9184 - acc: 0.6842\n",
      "Epoch 74/150\n",
      "625/625 [==============================] - 20s 32ms/step - loss: 0.9126 - acc: 0.6847\n",
      "Epoch 75/150\n",
      "625/625 [==============================] - 18s 29ms/step - loss: 0.9064 - acc: 0.6880\n",
      "Epoch 76/150\n",
      "625/625 [==============================] - 18s 29ms/step - loss: 0.9053 - acc: 0.6873\n",
      "Epoch 77/150\n",
      "625/625 [==============================] - 18s 29ms/step - loss: 0.9099 - acc: 0.6851\n",
      "Epoch 78/150\n",
      "625/625 [==============================] - 20s 32ms/step - loss: 0.8933 - acc: 0.6942\n",
      "Epoch 79/150\n",
      "625/625 [==============================] - 18s 30ms/step - loss: 0.8997 - acc: 0.6904\n",
      "Epoch 80/150\n",
      "625/625 [==============================] - 18s 30ms/step - loss: 0.8889 - acc: 0.6936\n",
      "Epoch 81/150\n",
      "625/625 [==============================] - 19s 31ms/step - loss: 0.8996 - acc: 0.6911\n",
      "Epoch 82/150\n",
      "625/625 [==============================] - 21s 33ms/step - loss: 0.8841 - acc: 0.6970\n",
      "Epoch 83/150\n",
      "625/625 [==============================] - 19s 30ms/step - loss: 0.8937 - acc: 0.6918\n",
      "Epoch 84/150\n",
      "625/625 [==============================] - 18s 30ms/step - loss: 0.8742 - acc: 0.6999\n",
      "Epoch 85/150\n",
      "625/625 [==============================] - 19s 30ms/step - loss: 0.8694 - acc: 0.7009\n",
      "Epoch 86/150\n",
      "625/625 [==============================] - 19s 30ms/step - loss: 0.8653 - acc: 0.7020\n",
      "Epoch 87/150\n",
      "625/625 [==============================] - 20s 31ms/step - loss: 0.8701 - acc: 0.6986\n",
      "Epoch 88/150\n",
      "625/625 [==============================] - 18s 30ms/step - loss: 0.8728 - acc: 0.6987\n",
      "Epoch 89/150\n",
      "625/625 [==============================] - 18s 29ms/step - loss: 0.8704 - acc: 0.7035\n",
      "Epoch 90/150\n",
      "625/625 [==============================] - 18s 30ms/step - loss: 0.8657 - acc: 0.7020\n",
      "Epoch 91/150\n",
      "625/625 [==============================] - 20s 32ms/step - loss: 0.8680 - acc: 0.7001\n",
      "Epoch 92/150\n",
      "625/625 [==============================] - 18s 30ms/step - loss: 0.8593 - acc: 0.7041\n",
      "Epoch 93/150\n",
      "625/625 [==============================] - 18s 30ms/step - loss: 0.8546 - acc: 0.7060\n",
      "Epoch 94/150\n",
      "625/625 [==============================] - 18s 29ms/step - loss: 0.8518 - acc: 0.7055\n",
      "Epoch 95/150\n",
      "625/625 [==============================] - 20s 32ms/step - loss: 0.8561 - acc: 0.7069\n",
      "Epoch 96/150\n",
      "625/625 [==============================] - 18s 29ms/step - loss: 0.8593 - acc: 0.7049\n",
      "Epoch 97/150\n",
      "625/625 [==============================] - 18s 29ms/step - loss: 0.8570 - acc: 0.7054\n",
      "Epoch 98/150\n",
      "625/625 [==============================] - 21s 33ms/step - loss: 0.8442 - acc: 0.7119\n",
      "Epoch 99/150\n",
      "625/625 [==============================] - 20s 31ms/step - loss: 0.8445 - acc: 0.7109\n",
      "Epoch 100/150\n",
      "625/625 [==============================] - 19s 30ms/step - loss: 0.8456 - acc: 0.7110\n",
      "Epoch 101/150\n",
      "625/625 [==============================] - 18s 29ms/step - loss: 0.8439 - acc: 0.7111\n",
      "Epoch 102/150\n",
      "625/625 [==============================] - 18s 29ms/step - loss: 0.8433 - acc: 0.7114\n",
      "Epoch 103/150\n",
      "625/625 [==============================] - 19s 30ms/step - loss: 0.8355 - acc: 0.7138\n",
      "Epoch 104/150\n",
      "625/625 [==============================] - 19s 31ms/step - loss: 0.8391 - acc: 0.7144\n",
      "Epoch 105/150\n",
      "625/625 [==============================] - 18s 29ms/step - loss: 0.8398 - acc: 0.7127\n",
      "Epoch 106/150\n",
      "625/625 [==============================] - 18s 29ms/step - loss: 0.8408 - acc: 0.7109\n",
      "Epoch 107/150\n",
      "625/625 [==============================] - 18s 29ms/step - loss: 0.8271 - acc: 0.7189\n",
      "Epoch 108/150\n",
      "625/625 [==============================] - 20s 32ms/step - loss: 0.8260 - acc: 0.7170\n",
      "Epoch 109/150\n",
      "625/625 [==============================] - 18s 29ms/step - loss: 0.8301 - acc: 0.7150\n",
      "Epoch 110/150\n",
      "625/625 [==============================] - 18s 29ms/step - loss: 0.8321 - acc: 0.7149\n",
      "Epoch 111/150\n",
      "625/625 [==============================] - 18s 29ms/step - loss: 0.8258 - acc: 0.7175\n",
      "Epoch 112/150\n",
      "625/625 [==============================] - 19s 31ms/step - loss: 0.8227 - acc: 0.7172\n",
      "Epoch 113/150\n",
      "625/625 [==============================] - 18s 29ms/step - loss: 0.8184 - acc: 0.7187\n",
      "Epoch 114/150\n",
      "625/625 [==============================] - 19s 31ms/step - loss: 0.8170 - acc: 0.7212\n",
      "Epoch 115/150\n",
      "625/625 [==============================] - 19s 30ms/step - loss: 0.8194 - acc: 0.7200\n",
      "Epoch 116/150\n",
      "625/625 [==============================] - 19s 30ms/step - loss: 0.8209 - acc: 0.7190\n",
      "Epoch 117/150\n",
      "625/625 [==============================] - 19s 30ms/step - loss: 0.8066 - acc: 0.7250\n",
      "Epoch 118/150\n",
      "625/625 [==============================] - 18s 29ms/step - loss: 0.8132 - acc: 0.7213\n",
      "Epoch 119/150\n",
      "625/625 [==============================] - 18s 29ms/step - loss: 0.8065 - acc: 0.7259\n",
      "Epoch 120/150\n",
      "625/625 [==============================] - 18s 29ms/step - loss: 0.8055 - acc: 0.7256\n",
      "Epoch 121/150\n",
      "625/625 [==============================] - 20s 31ms/step - loss: 0.8129 - acc: 0.7226\n",
      "Epoch 122/150\n",
      "625/625 [==============================] - 18s 29ms/step - loss: 0.8101 - acc: 0.7211\n",
      "Epoch 123/150\n",
      "625/625 [==============================] - 18s 29ms/step - loss: 0.8049 - acc: 0.7268\n",
      "Epoch 124/150\n",
      "625/625 [==============================] - 18s 29ms/step - loss: 0.8050 - acc: 0.7280\n",
      "Epoch 125/150\n",
      "625/625 [==============================] - 20s 32ms/step - loss: 0.8023 - acc: 0.7265\n",
      "Epoch 126/150\n",
      "625/625 [==============================] - 18s 30ms/step - loss: 0.8131 - acc: 0.7242\n",
      "Epoch 127/150\n",
      "625/625 [==============================] - 18s 29ms/step - loss: 0.8109 - acc: 0.7238\n",
      "Epoch 128/150\n",
      "625/625 [==============================] - 18s 30ms/step - loss: 0.8015 - acc: 0.7264\n",
      "Epoch 129/150\n",
      "625/625 [==============================] - 19s 30ms/step - loss: 0.7986 - acc: 0.7270\n",
      "Epoch 130/150\n",
      "625/625 [==============================] - 19s 31ms/step - loss: 0.7919 - acc: 0.7312\n",
      "Epoch 131/150\n",
      "625/625 [==============================] - 20s 32ms/step - loss: 0.7979 - acc: 0.7282\n",
      "Epoch 132/150\n",
      "625/625 [==============================] - 18s 29ms/step - loss: 0.7978 - acc: 0.7263\n",
      "Epoch 133/150\n",
      "625/625 [==============================] - 18s 29ms/step - loss: 0.7930 - acc: 0.7294\n",
      "Epoch 134/150\n",
      "625/625 [==============================] - 20s 32ms/step - loss: 0.7971 - acc: 0.7310\n",
      "Epoch 135/150\n",
      "625/625 [==============================] - 18s 29ms/step - loss: 0.7926 - acc: 0.7306\n",
      "Epoch 136/150\n",
      "625/625 [==============================] - 18s 29ms/step - loss: 0.7825 - acc: 0.7327\n",
      "Epoch 137/150\n",
      "625/625 [==============================] - 18s 29ms/step - loss: 0.7879 - acc: 0.7306\n",
      "Epoch 138/150\n",
      "625/625 [==============================] - 20s 32ms/step - loss: 0.7891 - acc: 0.7306\n",
      "Epoch 139/150\n",
      "625/625 [==============================] - 18s 29ms/step - loss: 0.7896 - acc: 0.7292\n",
      "Epoch 140/150\n",
      "625/625 [==============================] - 18s 29ms/step - loss: 0.7943 - acc: 0.7297\n",
      "Epoch 141/150\n",
      "625/625 [==============================] - 18s 30ms/step - loss: 0.7856 - acc: 0.7344\n",
      "Epoch 142/150\n",
      "625/625 [==============================] - 20s 31ms/step - loss: 0.7799 - acc: 0.7352\n",
      "Epoch 143/150\n",
      "625/625 [==============================] - 19s 30ms/step - loss: 0.7855 - acc: 0.7318\n",
      "Epoch 144/150\n",
      "625/625 [==============================] - 18s 30ms/step - loss: 0.7836 - acc: 0.7343\n",
      "Epoch 145/150\n",
      "625/625 [==============================] - 18s 30ms/step - loss: 0.7884 - acc: 0.7299\n",
      "Epoch 146/150\n",
      "625/625 [==============================] - 19s 30ms/step - loss: 0.7805 - acc: 0.7348\n",
      "Epoch 147/150\n",
      "625/625 [==============================] - 21s 34ms/step - loss: 0.7887 - acc: 0.7330\n",
      "Epoch 148/150\n",
      "625/625 [==============================] - 19s 31ms/step - loss: 0.7854 - acc: 0.7333\n",
      "Epoch 149/150\n",
      "625/625 [==============================] - 18s 29ms/step - loss: 0.7793 - acc: 0.7386\n",
      "Epoch 150/150\n",
      "625/625 [==============================] - 18s 29ms/step - loss: 0.7696 - acc: 0.7384\n"
     ]
    }
   ],
   "source": [
    "history = fin_model.fit_generator(datagen.flow(x_train, y_train_vec, batch_size=batch_size),\n",
    "                              steps_per_epoch=steps_per_epoch,\n",
    "                              epochs=150)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "-jwUrBrVqmQ5"
   },
   "source": [
    "### 3.2. Evaluate the model on the test set\n",
    "\n",
    "Do NOT used the test set until now. Make sure that your model parameters and hyper-parameters are independent of the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 68
    },
    "colab_type": "code",
    "id": "iXBYNbFXqmQ_",
    "outputId": "f1055056-4c2b-4040-ed64-9577ccdec12f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000/10000 [==============================] - 1s 99us/step\n",
      "loss = 0.7574639088630676\n",
      "accuracy = 0.7402\n"
     ]
    }
   ],
   "source": [
    "loss_and_acc = fin_model.evaluate(x_test, y_test_vec)\n",
    "print('loss = ' + str(loss_and_acc[0]))\n",
    "print('accuracy = ' + str(loss_and_acc[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "_gGO8zEsqmRD"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "name": "hw3cnn.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
